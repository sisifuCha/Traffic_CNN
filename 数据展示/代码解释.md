代码解释：

```shell
!echo y | unzip test_dataset.zip > log.log
!echo y | unzip train_dataset.zip > log.log
```

jupternotebook 用！开头 解压数据集

```python
import io, cv2
import math, json
import numpy as np
import pandas as pd
from PIL import Image

import matplotlib.pyplot as plt
import paddle
import paddle.nn.functional as F
import paddle.vision.transforms as T
from paddle.io import DataLoader, Dataset

import warnings
warnings.filterwarnings("ignore")

paddle.__version__
```

- pandas 表格数据处理库 有dataframe可使用
- numpy 科学计算包，最后会转为张量计算
- Ignore 警告使输出更整洁
- 打印paddle版本号

```python
# 读取数据集
train_json = pd.read_json('train.json')
train_json = train_json.sample(frac=1.0)

train_json['filename'] = train_json['annotations'].apply(lambda x: x['filename'])
train_json['label'] = train_json['annotations'].apply(lambda x: x['label'])
train_json.head()
```

- 用 Pandas 的 `read_json` 函数来读取名为 `train.json` 的文件。Pandas 会尝试解析 JSON 文件的结构，并将其加载到一个 DataFrame 对象中
- 随机抽取样例100%
- 从 `annotations` 列中提取出每个条目的 `'filename'` 值，并将其存储到一个新的、名为 `filename` 的顶层列中
  - lambda x: x['filename']该函数从字典 `x` 中提取键为 `'filename'` 的值。
  - 【`apply` 操作的结果（一个Pandas Series）被用来创建 `train_json` DataFrame 中名为 `filename` 的新列】
- 默认会显示 DataFrame 的前 5 行。

```python
plt.figure(figsize=(10, 10))
for idx in range(10):
    plt.subplot(1, 10, idx+1)
    img = cv2.imread(train_json['filename'].iloc[idx])
    plt.imshow(img)
    plt.xticks([]); plt.yticks([])
```

- 数据可视化
- 创建新图形，里面是1×10的子图（10张可视化）

```python
class WeatherDataset(Dataset):
    def __init__(self, df):
        super(WeatherDataset, self).__init__()
        self.df = df
    
        # 数据扩增方法
        self.transform = T.Compose([
            T.Resize(size=(128,128)),
            T.RandomCrop(size=(125, 125)),
            T.RandomRotation(10),
            T.RandomHorizontalFlip(),
            T.RandomVerticalFlip(),
            T.ToTensor(),
            T.Normalize(mean=0.5, std=0.5)
        ])

    def __getitem__(self, index):
        file_name = self.df['filename'].iloc[index]
        img = Image.open(file_name)
        img = self.transform(img)
        return img, paddle.to_tensor(self.df['label'].iloc[index])

    def __len__(self):
        return len(self.df)
# 训练集
train_dataset = WeatherDataset(train_json.iloc[:-500])
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# 验证集
val_dataset = WeatherDataset(train_json.iloc[-500:])
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)
```

- 图像变换序列transform 随机剪裁旋转，变成tensor数据增强
- 将原始的 `train_json` DataFrame 划分成两部分：大部分用于训练（,最后500条用于验证。

- `DataLoader` 负责在模型训练时，高效地、按批次地、可选择性地打乱顺序地提供数据。

```
from paddle.vision.models import resnet18

# 定义模型
class WeatherModel(paddle.nn.Layer):
    def __init__(self):
        super(WeatherModel, self).__init__()
        backbone = resnet18(pretrained=True)
        backbone.fc = paddle.nn.Identity()
        self.backbone = backbone
        self.fc1 = paddle.nn.Linear(512, 10)

    def forward(self, x):
        out = self.backbone(x)
        logits1 = self.fc1(out)
        return logits1
```

- 定义自己的全连接层（10分类）

```
# 优化器与损失函数
optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=0.0001)
criterion = paddle.nn.CrossEntropyLoss()

# 模型训练与验证
for epoch in range(0, 4):
    Train_Loss, Val_Loss = [], []
    Train_ACC1 = []
    Val_ACC1 = []
    
    model.train()
    for i, (x, y1) in enumerate(train_loader):
        pred1 = model(x)
        loss = criterion(pred1, y1)
        Train_Loss.append(loss.item())
        loss.backward()
        optimizer.step()
        optimizer.clear_grad()

        Train_ACC1.append((pred1.argmax(1) == y1.flatten()).numpy().mean())

    model.eval()
    for i, (x, y1) in enumerate(val_loader):
        pred1 = model(x)
        loss = criterion(pred1, y1)
        Val_Loss.append(loss.item())
        Val_ACC1.append((pred1.argmax(1) == y1.flatten()).numpy().mean())

    if epoch % 1 == 0:
        print(f'\nEpoch: {epoch}')
        print(f'Loss {np.mean(Train_Loss):3.5f}/{np.mean(Val_Loss):3.5f}')
        print(f'ACC {np.mean(Train_ACC1):3.5f}/{np.mean(Val_ACC1):3.5f}')
```

- adam优化器
- 交叉熵损失函数
- `pred1 = model(x)`: **前向传播**。将一批训练图像 `x` 输入模型，得到预测结果 `pred1` (logits)。
- `loss.backward()`: **反向传播**。计算损失函数关于模型所有可训练参数的梯度。
- `optimizer.step()`: **参数更新**。优化器根据计算得到的梯度以及其更新规则（Adam算法）来更新模型的参数。
- `optimizer.clear_grad()`: **梯度清零**。清除之前计算的梯度。这是必需的，因为 PaddlePaddle (和 PyTorch) 的梯度是累加的，如果不清零，下一次 `loss.backward()` 计算的梯度会累加到旧的梯度上。
- 在每轮训练中：
  - 先在训练集上进行训练（前向传播、计算损失、反向传播、参数更新）。
  - 然后在验证集上进行评估（前向传播、计算损失和准确率，但不更新参数）。
- 每轮结束后打印训练和验证的平均损失与准确率，以便监控模型的学习过程和性能。通过比较训练集和验证集的指标，可以判断模型是否出现过拟合或欠拟合。

```
import glob
test_df = pd.DataFrame({'filename': glob.glob('./test/*/*.jpg')})
test_df['label'] = 0
test_df = test_df.sort_values(by='filename')

test_dataset = WeatherDataset(test_df)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

model.eval()
pred = []

# 模型预测
for i, (x, y1) in enumerate(test_loader):
    pred1 = model(x)
    pred += pred1.argmax(1).numpy().tolist()

test_df['label'] = pred
```

- 查找文件地址 df占位符 进行预测
- model.eval() 评估模式不用dropout
- 遍历测试图像 给预测结果